\chapter{関連研究}
この章ではまず、既存のボードゲームAIについてAlphaZeroを中心に強化学習的枠組みからその理論を説明する。
次にAlpha Zeroの問題点とそれを補完する既存手法とその課題について述べる。



\section{強化学習}
強化学習はタスクを主体と環境のやり取りとして定式化する形でタスクに取り組む分野である。
状態($s$)と行動($a$)が次の状態$s'$と環境から与えられる報酬ｒが決定されると仮定する。その仮定の下、環境から与えられる報酬の合計(以下収益と記載)を最大化する。
報酬を大きくするためには状態sに応じて適切な(より大きな報酬をもらえる可能性が高い)行動を選択する必要がある。
ある状態である行動をとった場合の収益に対して見積もりをとり、見込まれる値が最も大きい行動を選択することでより大きな収益を獲得できると期待できる。
このようなある状態である行動を取った場合の収益の見積もりを$Q(s, a)$とした場合、
\begin{equation}
	a = {argmax}_{a'} Q(s, a')
\end{equation}
となるa選択することによって収益の最大化が期待される。また、ある状態から獲得できる収益の合計の予想値$V(s)$は、最適な行動aを取った場合の値として推定される。
\begin{equation}
	V(s) = Q(s, a)(a = {argmax}_{a'} Q(s, a'))
\end{equation}
強化学習手法によってタスクの最適化を図る際にはこの$V(s),Q(s, a')$を正しく推定することが直接的な目標となる。$V(s),Q(s, a)$は主体が実際に環境とやり取りを行う（タスクを実行していく）中で改善されていき、
Temporary Difference法\cite{oord2016wavenet}やMonte Carlo法\cite{oord2016wavenet}等が基本的な$V(s),Q(s, a)$の更新則である。また、DQN\cite{oord2016wavenet}やRainbow\cite{oord2016wavenet}等はニューラルネットワークを使用して$V(s),Q(s, a')$
を推定することでより高い性能を発揮している。



\subsection{ボードゲームへの応用}
ボードゲームでは通例、状態sは盤面の状況、行動はプレイヤーの選択、報酬はゲームの最後に勝敗として与えられる。
状態$s$(ゲームの状況)と行動$a$(プレイヤーの選択)によって盤面は次の状態$s'$に遷移し、次の行動$a'$(他のプレイヤーによる選択)を受け付ける、というサイクルにゲームの進行を定式化して表現することができる。
また、上述した強化学習における$V(s),Q(s, a)$の推定はそれぞれ「ある盤面はプレイヤーにとって勝利に近いのか」、「ある盤面においてある選択をした場合、プレイヤーはどれ程勝利に近くなるのか」を表現していると解釈される。
AlphaGo\cite{oord2016wavenet}やチェスのなんか強いやつ\cite{oord2016wavenet}、激指\cite{oord2016wavenet}では状態$s$は最新$N$ステップの盤面である。
盤面は行列に抽象化される。また、行動は次にプレイヤーが打つ箇所の座標となる。
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{./figure/ccl.png}
	\caption{AlphaZeroのチェスのやつとconnect4と強化学習の式を合わせていい感じに}
	\label{fig:ccl}
\end{figure}
本論文で使用したalphazero baselineにおける入力は最新の盤面の状態を空白を0, 先番(赤)の石の位置を1, 後ろ
番（黄）の位置を-1として抽象化した6$\times$7の行列となる。
また、後述するconnect4のルール上の制約により次にプレイヤーが打つ箇所(行動)は列の数と同数の7つに限定される。

\subsection{AlphaZero}
AlphaZeroは2016年に登場し、元世界チャンピオンであるイ・セドルに対して四勝一敗の成績を収めたAlphaGoの汎用版である。
AlphaZeroは先述の$V(s),Q(s, a)$を推定する際にニューラルネットワークとモンテカルロ木探索システムを使用する。
\paragraph{ニューラルネットワーク}
AlphaZero内のニューラルネットワークに対する入力は最新$N$ステップの盤面($\left\{ s_{-N+1}, ..., s_0 \right\}$,$s_{-i}$は$i$ステップ前の盤面,$s_{0}$は現在の盤面)であり、出力は方策$P(\left\{ s_{-N+1}, ..., s_0 \right\})$と
局面評価（後で確認）$V(s_0)$の二種類である。\
ネットワークの構成は 層の残差結合ネットワークである。方策は「現在の状況$s_0$から次にどこを選択すべきか」を表現しており、次に選択すべき座標を確率分布の形式で表現する。
alphazero baselineにおける選択肢は列の数と等しい7であるため、1$\times$7の行列となる。方策内の値が大きさがAIによるその着手の評価と解釈され、成分が大きい座標を次に選択することが推奨される。例えば方策が
$\left\{0, 0.1, 0.2, 0, 0, 0.7, 0.8\right\}$であるとき、方策中の最も大きい成分は7番目の0.8であるため、プレイヤーは次に７列目を選択する事が推奨される。
また、局面評価$V(s_0)$は「現在の状況$s_0$は勝利に近いのか」を表現しており、値が上限に近ければ近い程、現在の状況$s_0$が次の着手を選択するプレイヤーにとっての勝利に近いことを表している。

訓練とかどうするん？
\begin{figure}[t]
	\centering
	\includegraphics[width=\linewidth]{./figure/ccl.png}
	\caption{方策と局面評価、あと残差結合の図も}
	\label{fig:ccl}
\end{figure}
\paragraph{モンテカルロ木探索}
モンテカルロ木探索ではニューラルネットワークから得た方策$P(s)(以下s=\left\{ s_{-N+1}, ..., s_0 \right\}とおく)$と局面評価$V(s)$をシミュレーション
によって改善する。
モンテカルロ木探索ではシミュレーションによって各状態$s$をノード、各行動$a$を枝とした決定木を構築する。
最終的に各ノード$s$から派生する各行動$a$の分布$\left\{a_0, a_1, ..., a_n\right\}$が改善された方策となる。
一方、局面評価もまたモンテカルロ木探索により決定木が拡張されるなかで更新される。以下に決定木と局面評$V(s)$の更新アルゴリズムを示す。
表アルゴリズム\ref{table:result-1}
step1ではまず、探索の開始地点となるノード$s$を決定する。次にStep2:再帰部分では以下の処理を再起的に呼び出す。
\begin{enumerate}
	\item ノードを探索したことがない場合
	ニューラルネットワークから出力された方策$P(s)$と局面評価$V(s)$を返却する
	\item ノードを探索したことがある場合
	以下のpuctスコアに従い、子ノード$s_c$を選び、$s_c$に対して探索を行いその結果である$P(s_e), V(s_e)(s_eは再帰処理の結果たどり着く
	決定木の端のノード)$を受け取る。
\end{enumerate}

$P(s_e), V(s_e)$を用いて
パラメータをこんなふうに更新する。

このようなプロセスによって構築された決定木を用いてモデルは対戦を行う。
\subsection{AlphaZeroの問題点}
StockFish\cite{oord2016wavenet}やなんか\cite{oord2016wavenet}などの従来のボードゲームAIはそのゲーム固有の知識(ドメイン知識)に基づくものが多く、「ある条件を満たすときにある選択をする」と言ったようにその挙動をルールとして表すことが可能である。
一方でこのニューラルネットワーク+木探索の手法では
人間がネットワークから得られる情報は方策と局面評価のみである。その他のパラメータも観察可能ではあるが、その方策や局面評価の根拠を得ることができない、つまりAlphsZeroの問題点として説明性の欠如が挙げられるのである。
説明性の欠如はAIの判断に対する責任の不在を意味し、優れた性能を持つシステムのより、ハイレベル、ハイリスクなタスクへの実用化に対する障害となる。



\section{XAI}

\subsection{概要}
XAIとはexplainable AI(説明可能AI)の略語であり、AIを人間に対して説明可能なものにする、もしくは説明可能なAIを構築する領域である。
本論文はAIの判断根拠として先読みを示す意味でXAIの分野に属する研究であると言える。
ここでの「説明可能性」の語は「人間に理解できる形での説明を与える能力」\cite{oord2016wavenet}と定義され、
「いつ、どのような、どのように」説明を与えるかによってさらに細かく分類される。
「いつ」、つまりどの時点で説明を与えるか、に関しては既存のネットワークに対して新たに説明を加える「事後的」説明と初めから動作の根拠を示せるようにネットワークやシステムを構築する「事前的」説明
に分類できる\cite{oord2016wavenet}。
「どのような」、つまり説明の内容については
が存在する。
また、「どのように」つまり説明を表現する形態としてはsaliency map, Grad-CAM\cite{oord2016wavenet}といった視覚的な可視化や、あとで
\cite{oord2016wavenet}といった文章生成等が存在する。
本論文において構築するシステムは「事後的」「局所的」「視覚的」説明を提供する。

Wavenet\cite{oord2016wavenet}は
音声波形を時系列データとして自己回帰モデルで学習することによって，人間の声のような自然な音声を生成することができる．
時点$t$における観測値を$x_t$，$\bm{x} = \left\{ x_1, ..., x_T \right\}$を観測値の全体集合とする．このとき，波形の同時確率は条件付き確率の積として
以下のよう表現される．
\begin{equation}
	p(\bm{x}) = \prod_{t=1}^T p(x_t | x_1, ..., x_{t-1})
\end{equation}

つまり，$x_t$は前時点の全てにおけるサンプルに条件づけられる．
また、本論文は
XAI分野の中でも特に強化学習方面に対して説明を加える領域をXRL(explainable Reinforcement learning)と呼ぶ。XRLの試みは様々な強化学習システムを対象とし、
\subsection{ボードゲームにおけるXAI}
グーグルのやつはチェスにおける人間の知識がAlphaZeroにどれだけ反映されているかを訓練段階やネットワークの深さなどの多様な指標で調査している。\cite{oord2016wavenet}
ディコードチェス\cite{oord2016wavenet}はAIの着手に対して、そのゲーム固有の知識（以下ドメイン知識と記載する）を用いて解説を生成する 
しかし、このようなドメイン知識が必ずしもAIの挙動と相関が無いことも指摘されている。\cite{oord2016wavenet}
AIによる画像分類の可視化手法であるサリエンシーマップ\cite{oord2016wavenet}やGrad\_CAM\cite{oord2016wavenet}を強化学習に用いる例も存在する。しかしそれらのニューラルネットワークの活性を根拠とした指標は木探索部分との繋がりが弱く、最終的に決定木を用いて意思決定を行うシステムの動作根拠を直接的に説明できない。
また、これらの画像分類用の手法は
\begin{itemize}
	\item 本来ゲームに存在する時系列の要素を説明に含められない
	\item 時系列を無視してゲーム画面や盤面の一部を変更する必要がある
\end{itemize}
という問題点が存在する。

\subsection{contrastive explanation}
上述の問題点を解決するために、本論文ではAlphaZeroが構築する決定木を用いたcontrastive explanation（対象説明、比較説明）を提供する。
contrastive explanationは事象を説明する際の方法論の一つであり、
ある事象$a$が起こった際にその理由を直接説明する代わりに「他の事象$\bar{a}$が起こらなかった理由」を説明することで間接的にある事象$a$の原因を説明するアイデアである。
 では言語の何か \cite{oord2016wavenet}ではニューラルネットワークの判断を決定木に近似した上でニューラルネットワークの判断$a$から派生する予想と別の判断$b$から派生する予想を同時に提示する。
ロボット\cite{oord2016wavenet}の、ではある時点$t$で異なる行動を選んだ場合の結果の違いを文章で説明する
アミア\cite{oord2016wavenet}では異なるシステム間の特徴をユーザーに示す目的でcontrastive explanationが用いられている。

\subsection{ボードゲーム学習支援}
本論文が提案する手法は高いパフォーマンスを発揮するAIの動作を人間に理解させることを目標としており、学習支援の面を持つ。
既存のAIを用いたボードゲーム学習支援システムとしてはドメインみたいなやつをいくつか
が存在し、決定木の先読みを用いて文章生成や解説生成を行う。
しかし「ボードゲームに対するXAI」の段での内容と同様にその多くがゲームのドメイン知識に依存しており、指導の内容も人間の知識に依存したものになってしまうという欠点がある。



